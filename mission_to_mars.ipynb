{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission to Mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will build a web application that scrapes various websites for data related to the Mission to Mars and displays the information in a single HTML page. The following outlines what you need to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Scraping\n",
    "Complete your initial scraping using Jupyter Notebook, BeautifulSoup, Pandas, and Requests/Splinter.\n",
    "\n",
    "Create a Jupyter Notebook file called mission_to_mars.ipynb and use this to complete all of your scraping and analysis tasks. The following outlines what you need to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize browser\n",
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize browser\n",
    "browser = init_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the NASA Mars news site\n",
    "url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "browser.visit(url)\n",
    "\n",
    "# Scrape page into soup\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. \n",
    "# Assign the text to variables that you can reference later.\n",
    "news = soup.select(\".grid_layout .list_text\")[0]\n",
    "# news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Storm Chasers' on Mars Searching for Dusty Secrets\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article title\n",
    "news.a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Scientists with NASA's Mars orbiters have been waiting years for an event like the current Mars global dust storm.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article text\n",
    "\n",
    "#  max_temp = forecast_today.find(\"span\", class_=\"temp-max\").text\n",
    "text = news.find(\"div\", class_=\"article_teaser_body\").text\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize browser\n",
    "# browser = init_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the url for JPL Featured Space Image here.\n",
    "url2 = \"https://www.jpl.nasa.gov/spaceimages/index.php?category=Mars\"\n",
    "# Use splinter to navigate the site and find the image url for the current Featured Mars Image \n",
    "# and assign the url string to a variable called featured_image_url.\n",
    "browser.visit(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = browser.find_by_id('full_image')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape page into soup\n",
    "html2 = browser.html\n",
    "soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "# soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/spaceimages/images/mediumsize/PIA16021_ip.jpg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use splinter to navigate the site and find the image url for the current Featured Mars \n",
    "# Image and assign the url string to a variable called featured_image_url.\n",
    "pic_url = soup2.select(\".fancybox-image\")[0]['src']\n",
    "\n",
    "pic_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA16021_ip.jpg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to save a complete url string for this image.\n",
    "featured_image_url = \"https://www.jpl.nasa.gov\" + pic_url\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Mars Weather twitter account here and scrape the latest Mars weather tweet from the\n",
    "# page. Save the tweet text for the weather report as a variable called mars_weather.\n",
    "\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_CONSUMER_KEY = os.getenv(\"TWITTER_CONSUMER_KEY\")\n",
    "TWITTER_CONSUMER_SECRET=os.getenv(\"TWITTER_CONSUMER_SECRET\")\n",
    "TWITTER_ACCESS_TOKEN=os.getenv(\"TWITTER_ACCESS_TOKEN\")\n",
    "TWITTER_ACCESS_TOKEN_SECRET=os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET)\n",
    "auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # target_user = \"MarsWxReport\"\n",
    "\n",
    "            # tweet = api.user_timeline(target_user, count=1)\n",
    "            # tweet = tweet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @icancallubetty: L-2 years. #Mars2020'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            # mars_weather = tweet[\"text\"]\n",
    "            # mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, the last tweet that this user has is a retweet of something not related to Mars weather so I'm going\n",
    "# to have to scrape twitter and not use the twitter API.\n",
    "\n",
    "twitter_url = \"https://twitter.com/MarsWxReport\"\n",
    "browser.visit(twitter_url)\n",
    "\n",
    "# Scrape page into soup\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sol 2108 (2018-07-12), Sunny, high -24C/-11F, low -65C/-84F, pressure at 8.06 hPa, daylight 05:19-17:27\n"
     ]
    }
   ],
   "source": [
    "mars_weather_tweet = soup.find('div', attrs={\"class\": \"tweet\", \"data-screen-name\": \"MarsWxReport\"})\n",
    "mars_weather_tweet\n",
    "\n",
    "tweet = mars_weather_tweet.find('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text').text\n",
    "\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                      0                              1\n",
       " 0  Equatorial Diameter:                       6,792 km\n",
       " 1       Polar Diameter:                       6,752 km\n",
       " 2                 Mass:  6.42 x 10^23 kg (10.7% Earth)\n",
       " 3                Moons:            2 (Phobos & Deimos)\n",
       " 4       Orbit Distance:       227,943,824 km (1.52 AU)\n",
       " 5         Orbit Period:           687 days (1.9 years)\n",
       " 6  Surface Temperature:                  -153 to 20 Â°C\n",
       " 7         First Record:              2nd millennium BC\n",
       " 8          Recorded By:           Egyptian astronomers]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including \n",
    "# Diameter, Mass, etc.\n",
    "# Use Pandas to convert the data to a HTML table string.\n",
    "\n",
    "url = \"https://space-facts.com/mars/\"\n",
    "\n",
    "table = pd.read_html(url)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the USGS Astrogeology site here to obtain high resolution images for each of Mar's hemispheres.\n",
    "# browser = init_browser()\n",
    "\n",
    "hemis_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "browser.visit(hemis_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape page into soup\n",
    "html = browser.html\n",
    "soup3 = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/search/map/Mars/Viking/valles_marineris_enhanced'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isolate the class\n",
    "soup3.select(\".description\")[3].a[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/search/map/Mars/Viking/cerberus_enhanced\n",
      "/search/map/Mars/Viking/schiaparelli_enhanced\n",
      "/search/map/Mars/Viking/syrtis_major_enhanced\n",
      "/search/map/Mars/Viking/valles_marineris_enhanced\n"
     ]
    }
   ],
   "source": [
    "# Loop to pull out the partial urls\n",
    "hemispheres = []\n",
    "\n",
    "for i in range(4):\n",
    "    hemi = soup3.select(\".description\")[i].a[\"href\"]\n",
    "    print(hemi)\n",
    "    hemispheres.append(hemi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced\n",
      "https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced\n"
     ]
    }
   ],
   "source": [
    "# Loop to add the partial URL to the original and append it to a list\n",
    "image_url = []\n",
    "for h in hemispheres:\n",
    "    image = \"https://astrogeology.usgs.gov\"+h\n",
    "    image_url.append(image)\n",
    "    print(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to go to each of the URLs and scrape it for the full images uand the title\n",
    "hemi_url = []\n",
    "hemi_title = []\n",
    "for url in image_url:\n",
    "#     browser = init_browser()\n",
    "    browser.visit(url)\n",
    "    # Scrape page into soup\n",
    "    html = browser.html\n",
    "    soup4 = BeautifulSoup(html, 'html.parser')\n",
    "    href = soup4.select(\".downloads\")[0].a[\"href\"]\n",
    "    hemi_url.append(href)\n",
    "    title = soup4.find(\"h2\", class_=\"title\").text\n",
    "    hemi_title.append(title)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg',\n",
       " 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg',\n",
       " 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg',\n",
       " 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hemi_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a list of dictionaries with my lists of urls and image titles.\n",
    "hemisphere_info = []\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    hemisphere_info.append({\n",
    "        \"title\": hemi_title[i],\n",
    "        \"url\": hemi_url[i]\n",
    "    })\n",
    "    \n",
    "hemisphere_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the costa rica page\n",
    "# put all of the data you scraped into a dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
